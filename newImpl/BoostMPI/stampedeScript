#!/bin/bash
#----------------------------------------------------
# Generic SLURM script -- MPI Hello World
#
#----------------------------------------------------
#SBATCH -J myjob          # Job name
#SBATCH -o myjob.%j.out   # stdout; %j expands to jobid
#SBATCH -e myjob.%j.err   # stderr; skip to combine stdout and stderr
#SBATCH -p development    # queue
#SBATCH -N 1              # Number of nodes, not cores (68 cores/node for KNL, 48 for skx)
#SBATCH -n 3              # Total number of MPI tasks (if omitted, n=N)
#SBATCH -t 00:05:00       # max time

#SBATCH --mail-user=username@tacc.utexas.edu
#SBATCH --mail-type=ALL

#SBATCH -A CMPS-5433-MWSU  # class project/account code;
                          # necessary if you have multiple project accounts

#INSTRUCTIONS TO USE SCRIPT
       #sbatch [scriptName] --- send job to queue
       #scancel [jobid] ---- cancel job
       #showq -u [your username] ---- show your jobs on the queue

module restore system
module load fftw3         # Load any necessary modules (these are examples)
module list

ibrun ./CacheDNASequencingBoost /home1/04352/aenem/repo/DNASequencing/newImpl/common/inputFiles/5.dat    # Use ibrun only for MPI codes. Don't use mpirun or srun.
                        # Do not add "-n" or "-np" options here. SLURM infers the
                        # process count from the "-N" and "-n" directives above.
